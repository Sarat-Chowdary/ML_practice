{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a37853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0653b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv\n",
    "iris_values = []\n",
    "\n",
    "f = open('iris.csv') \n",
    "next(f)\n",
    "data = [line.split(',') for line in f.readlines()]\n",
    "for row in data:\n",
    "    a = []\n",
    "    for val in row:\n",
    "        a.append(float(val))\n",
    "    iris_values.append(a)\n",
    "f.close()\n",
    "\n",
    "random.shuffle(iris_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be335f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 4. 2. 0. 0.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [7. 3. 5. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [7. 3. 5. 2. 2.]\n",
      " [5. 3. 4. 1. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 4. 1. 0. 0.]\n",
      " [6. 4. 2. 0. 0.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [8. 3. 7. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [7. 3. 5. 1. 1.]\n",
      " [4. 2. 1. 0. 0.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 6. 1. 2.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 3. 5. 1. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 3. 5. 2. 1.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 4. 1. 0. 0.]\n",
      " [8. 3. 7. 2. 2.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [6. 3. 4. 2. 1.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [5. 2. 3. 1. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [7. 3. 5. 1. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 3. 4. 2. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [7. 2. 6. 2. 2.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [7. 3. 5. 1. 1.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [7. 4. 6. 2. 2.]\n",
      " [7. 3. 5. 2. 2.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [8. 4. 7. 2. 2.]\n",
      " [6. 3. 5. 1. 1.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [7. 3. 5. 2. 1.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [8. 3. 6. 2. 2.]\n",
      " [6. 2. 5. 2. 2.]\n",
      " [8. 3. 7. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [6. 4. 1. 0. 0.]\n",
      " [6. 2. 4. 2. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [4. 3. 1. 0. 0.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [7. 3. 4. 1. 1.]\n",
      " [6. 3. 5. 2. 1.]\n",
      " [6. 3. 4. 2. 1.]\n",
      " [6. 4. 2. 0. 0.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [5. 4. 2. 1. 0.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [5. 2. 4. 1. 1.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [6. 3. 5. 2. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [4. 3. 1. 0. 0.]\n",
      " [5. 3. 4. 2. 1.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [6. 3. 4. 2. 1.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [6. 2. 5. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 2. 5. 2. 2.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [4. 3. 1. 0. 0.]\n",
      " [6. 2. 5. 2. 1.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [7. 3. 5. 2. 1.]\n",
      " [6. 3. 5. 2. 1.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [5. 2. 4. 2. 2.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [6. 3. 4. 2. 1.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [4. 3. 1. 0. 0.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [7. 3. 5. 2. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [7. 3. 4. 1. 1.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [5. 2. 3. 1. 1.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [5. 2. 3. 1. 1.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [8. 4. 6. 2. 2.]\n",
      " [6. 3. 5. 1. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 3. 4. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# print(iris_values)\n",
    "\n",
    "# discretising the values using the np.round_ function\n",
    "discrete_iris = np.round_(iris_values)\n",
    "print(discrete_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9bc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the priors for different classes\n",
    "def calculate_prior(df, Y):\n",
    "    classes = sorted(list(df[Y].unique()))\n",
    "    prior = []\n",
    "    for i in classes:\n",
    "        prior.append(len(df[df[Y]==i])/len(df))\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da2ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the likelihood (1st term in bayes theorem)\n",
    "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
    "    feat = list(df.columns)\n",
    "    df = df[df[Y]==label]\n",
    "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
    "    p_x_given_y = (1 / (np.sqrt(2 * np.pi) * max(0.001,std))) *  np.exp(-((feat_val-mean)**2 / (2 * max(0.001,std**2) )))\n",
    "#     print(std)\n",
    "#     print(feat_val-mean)\n",
    "    return p_x_given_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8437856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main code for predicting multiple samples at a time\n",
    "def naive_bayes_gaussian(df, X, Y):\n",
    "    # getting names of features using df dataframe column names\n",
    "    features = list(df.columns)[:-1]\n",
    "    # calculating prior\n",
    "    prior = calculate_prior(df, Y)\n",
    "    \n",
    "    Y_pred = []\n",
    "    # loop over every data sample\n",
    "    count=0\n",
    "    for x in X:\n",
    "        # calculate likelihood\n",
    "        labels = sorted(list(df[Y].unique()))\n",
    "        likelihood = [1]*len(labels)\n",
    "        \n",
    "        for j in range(len(labels)):\n",
    "            for i in range(len(features)):\n",
    "                count+=1\n",
    "#                 print(count)\n",
    "                likelihood[j] *= calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
    "\n",
    "        # calculating posterior probability by using bayes theorem  \n",
    "        post_prob = [1]*len(labels)\n",
    "        for j in range(len(labels)):\n",
    "            post_prob[j] = likelihood[j] * prior[j]\n",
    "\n",
    "        Y_pred.append(np.argmax(post_prob))\n",
    "        # returning max values of all possible classes\n",
    "    return np.array(Y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dd50d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 0 0]\n",
      " [0 9 4]\n",
      " [0 0 8]]\n",
      "0.8727272727272727\n"
     ]
    }
   ],
   "source": [
    "# spliting the dataset into train and test (120 and 30)\n",
    "# plotting the predictins as a confusion matrix\n",
    "# also using the f1 score as a metric to check how good the model is\n",
    "# this is the dicretised data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(discrete_iris, test_size=.2, random_state=41)\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i in range(len(test)):\n",
    "    test_x.append([test[i][0], test[i][1], test[i][2],\n",
    "                      test[i][3]])\n",
    "    test_y.append(test[i][4])\n",
    "\n",
    "train_pd = pd.DataFrame(train, columns=['a', 'b', 'c', 'd', 'target'])\n",
    "\n",
    "# print(train_pd.head())\n",
    "\n",
    "Y_pred = naive_bayes_gaussian(train_pd, X=test_x, Y=\"target\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "print(confusion_matrix(test_y, Y_pred))\n",
    "print(f1_score(test_y, Y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a425aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  0  0]\n",
      " [ 0 10  3]\n",
      " [ 0  0  8]]\n",
      "0.9038901601830664\n"
     ]
    }
   ],
   "source": [
    "# plotting the predictins as a confusion matrix\n",
    "# non-discretised data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(iris_values, test_size=.2, random_state=41)\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i in range(len(test)):\n",
    "    test_x.append([test[i][0], test[i][1], test[i][2],\n",
    "                      test[i][3]])\n",
    "    test_y.append(test[i][4])\n",
    "\n",
    "train_pd = pd.DataFrame(train, columns=['a', 'b', 'c', 'd', 'target'])\n",
    "\n",
    "# print(train_pd.head())\n",
    "\n",
    "Y_pred = naive_bayes_gaussian(train_pd, X=test_x, Y=\"target\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "print(confusion_matrix(test_y, Y_pred))\n",
    "print(f1_score(test_y, Y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c26851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the training data .dat file\n",
    "train_set = []\n",
    "\n",
    "f = open('pp_tra.dat') \n",
    "next(f)\n",
    "data = [line.split(' ') for line in f.readlines()]\n",
    "for row in data:\n",
    "    a = []\n",
    "    for val in row:\n",
    "        a.append(int(val))\n",
    "    train_set.append(a)\n",
    "f.close()\n",
    "\n",
    "random.shuffle(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe6ceb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the testing data .dat file\n",
    "test_set = []\n",
    "\n",
    "f = open('pp_tes.dat') \n",
    "next(f)\n",
    "data = [line.split(' ') for line in f.readlines()]\n",
    "for row in data:\n",
    "    a = []\n",
    "    for val in row:\n",
    "        a.append(int(val))\n",
    "    test_set.append(a)\n",
    "f.close()\n",
    "\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9d3d822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6669\n",
      "3332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m train_pd \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(train_set)\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m192\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(train_pd.head())\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnaive_bayes_gaussian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, f1_score\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(test_y, Y_pred))\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mnaive_bayes_gaussian\u001b[1;34m(df, X, Y)\u001b[0m\n\u001b[0;32m     17\u001b[0m                 count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#                 print(count)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m                 likelihood[j] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_likelihood_gaussian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# calculate posterior probability (numerator only)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         post_prob \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(labels)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mcalculate_likelihood_gaussian\u001b[1;34m(df, feat_name, feat_val, Y, label)\u001b[0m\n\u001b[0;32m      2\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      3\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[df[Y]\u001b[38;5;241m==\u001b[39mlabel]\n\u001b[1;32m----> 4\u001b[0m     mean, std \u001b[38;5;241m=\u001b[39m df[feat_name]\u001b[38;5;241m.\u001b[39mmean(), \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeat_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     p_x_given_y \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0.001\u001b[39m,std))) \u001b[38;5;241m*\u001b[39m  np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m((feat_val\u001b[38;5;241m-\u001b[39mmean)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0.001\u001b[39m,std\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) )))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     print(std)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     print(feat_val-mean)\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\pandas\\core\\generic.py:10628\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.std\u001b[1;34m(self, axis, skipna, level, ddof, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10610\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m  10611\u001b[0m     _num_ddof_doc,\n\u001b[0;32m  10612\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn sample standard deviation over requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10626\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  10627\u001b[0m ):\n\u001b[1;32m> 10628\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mstd(\u001b[38;5;28mself\u001b[39m, axis, skipna, level, ddof, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\pandas\\core\\generic.py:10320\u001b[0m, in \u001b[0;36mNDFrame.std\u001b[1;34m(self, axis, skipna, level, ddof, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstd\u001b[39m(\n\u001b[0;32m  10318\u001b[0m     \u001b[38;5;28mself\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, numeric_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  10319\u001b[0m ):\n\u001b[1;32m> 10320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function_ddof(\n\u001b[0;32m  10321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanstd, axis, skipna, level, ddof, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  10322\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\pandas\\core\\generic.py:10299\u001b[0m, in \u001b[0;36mNDFrame._stat_function_ddof\u001b[1;34m(self, name, func, axis, skipna, level, ddof, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10289\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m  10290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the level keyword in DataFrame and Series aggregations is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m  10291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in a future version. Use groupby \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10294\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m  10295\u001b[0m     )\n\u001b[0;32m  10296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_by_level(\n\u001b[0;32m  10297\u001b[0m         name, axis\u001b[38;5;241m=\u001b[39maxis, level\u001b[38;5;241m=\u001b[39mlevel, skipna\u001b[38;5;241m=\u001b[39mskipna, ddof\u001b[38;5;241m=\u001b[39mddof\n\u001b[0;32m  10298\u001b[0m     )\n\u001b[1;32m> 10299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\n\u001b[0;32m  10301\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\pandas\\core\\series.py:4392\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   4388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   4389\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not implement numeric_only.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4390\u001b[0m     )\n\u001b[0;32m   4391\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 4392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(delegate, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpu\\lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# `mask` is not recognised by bottleneck, would raise\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m#  TypeError if called\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m bn_func(values, axis\u001b[38;5;241m=\u001b[39maxis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# prefer to treat inf/-inf as NA, but must compute the func\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# twice :(\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_infs(result):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# running the same bayes classifier as before \n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i in range(len(test_set)):\n",
    "    a = []\n",
    "    for j in range(len(test_set[1])):\n",
    "        a.append(test_set[i][j])\n",
    "    test_y.append(test_set[i][-1])\n",
    "    test_x.append(a)\n",
    "\n",
    "train_pd = pd.DataFrame(train_set).rename(columns=str).rename(columns={'192': 'target'})\n",
    "\n",
    "# print(train_pd.head())\n",
    "\n",
    "Y_pred = naive_bayes_gaussian(train_pd, X=test_x, Y=\"target\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "print(confusion_matrix(test_y, Y_pred))\n",
    "print(f1_score(test_y, Y_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
